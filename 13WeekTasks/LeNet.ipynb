{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhdM0UQF8VDljxvGGcZfKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarkKillX/MachineLearningTasks/blob/main/13WeekTasks/LeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YT0s6dp7Yv1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adc9fd6-7cf1-4ab4-95b3-59ecbcb54930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 155899368.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 36106019.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 45163235.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18212742.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch 1/5, Loss: 0.28478929496852795\n",
            "Epoch 2/5, Loss: 0.07962196020395565\n",
            "Epoch 3/5, Loss: 0.056075069353854194\n",
            "Epoch 4/5, Loss: 0.044454039288322325\n",
            "Epoch 5/5, Loss: 0.03639277283752363\n",
            "Test Accuracy: 0.9876\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the LeNet-5 architecture for MNIST\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = LeNet5().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Testing\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "        total_correct += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries:\n",
        "\n",
        "import torch: Mengimpor pustaka PyTorch.\n",
        "import torch.nn as nn: Mengimpor modul neural network dari PyTorch.\n",
        "import torch.optim as optim: Mengimpor modul optimisasi (untuk algoritma optimisasi).\n",
        "import torchvision: Mengimpor pustaka yang berguna untuk pengolahan citra dan dataset populer.\n",
        "import torchvision.transforms as transforms: Mengimpor modul untuk transformasi data citra.\n",
        "from torch.utils.data import DataLoader: Mengimpor DataLoader yang digunakan untuk memudahkan pengolahan data dalam batch.\n",
        "Set Device:\n",
        "\n",
        "Menentukan penggunaan GPU jika tersedia, jika tidak maka menggunakan CPU.\n",
        "Definisi Arsitektur LeNet-5:\n",
        "\n",
        "Membuat kelas LeNet5 yang merupakan arsitektur jaringan saraf untuk dataset MNIST. Ini termasuk beberapa lapisan konvolusi (Conv2d), lapisan aktivasi ReLU (ReLU), dan lapisan pooling (MaxPool2d), serta lapisan terhubung penuh (Linear).\n",
        "Memuat Dataset MNIST:\n",
        "\n",
        "Melakukan transformasi pada data (resize dan konversi ke tensor) dan memuat dataset MNIST untuk pelatihan dan pengujian.\n",
        "Inisialisasi DataLoader:\n",
        "\n",
        "Mengatur DataLoader untuk pelatihan dan pengujian.\n",
        "Inisialisasi Model, Fungsi Loss, dan Optimizer:\n",
        "\n",
        "Membuat instance dari model LeNet-5, menetapkan fungsi loss (CrossEntropyLoss) dan optimizer (Adam).\n",
        "Pelatihan (Training):\n",
        "\n",
        "Melakukan iterasi pada setiap epoch. Untuk setiap batch di DataLoader pelatihan, melakukan forward pass, menghitung loss, melakukan backward pass (backpropagation), dan memperbarui bobot model dengan optimizer.\n",
        "Pengujian (Testing):\n",
        "\n",
        "Menetapkan model ke mode evaluasi dan menghitung akurasi model pada dataset pengujian."
      ],
      "metadata": {
        "id": "BjHm41KEoNxf"
      }
    }
  ]
}